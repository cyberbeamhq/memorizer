# Memorizer Framework Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# PostgreSQL connection string
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/memorizer

# Connection pool settings
DB_MIN_CONNECTIONS=1
DB_MAX_CONNECTIONS=10

# =============================================================================
# VECTOR DATABASE CONFIGURATION
# =============================================================================
# Vector database provider: mock, pinecone, weaviate, chroma, pgvector
VECTOR_DB_PROVIDER=mock

# Pinecone configuration
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=memorizer

# Weaviate configuration
WEAVIATE_URL=http://localhost:8080
WEAVIATE_API_KEY=your_weaviate_api_key
WEAVIATE_CLASS_NAME=Memory

# Chroma configuration
CHROMA_HOST=localhost
CHROMA_PORT=8000
CHROMA_COLLECTION_NAME=memories

# pgvector configuration (uses same DATABASE_URL)
PGVECTOR_EXTENSION=vector

# =============================================================================
# EMBEDDING PROVIDERS
# =============================================================================
# Primary embedding provider: openai, cohere, huggingface, mock
EMBEDDING_PROVIDER=openai

# OpenAI configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_COMPRESSION_MODEL=gpt-4o-mini

# Cohere configuration
COHERE_API_KEY=your_cohere_api_key
COHERE_EMBEDDING_MODEL=embed-english-v3.0
COHERE_COMPRESSION_MODEL=command

# HuggingFace configuration
HUGGINGFACE_API_KEY=your_huggingface_api_key
HUGGINGFACE_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
HUGGINGFACE_COMPRESSION_MODEL=microsoft/DialoGPT-medium

# =============================================================================
# LLM PROVIDERS (FOR COMPRESSION AND GENERATION)
# =============================================================================
# Primary LLM provider: openai, anthropic, groq, openrouter, ollama, custom, mock
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini  # Fallback model name (provider-specific models take precedence)

# OpenAI LLM configuration
OPENAI_API_KEY=your_openai_api_key
OPENAI_MODEL=gpt-4o-mini  # Specific model for OpenAI
OPENAI_BASE_URL=https://api.openai.com/v1
# Available models: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-4, gpt-3.5-turbo

# Anthropic Claude configuration
ANTHROPIC_API_KEY=your_anthropic_api_key
ANTHROPIC_MODEL=claude-3-sonnet-20240229  # Specific model for Anthropic
# Available models: claude-3-opus-20240229, claude-3-sonnet-20240229, claude-3-haiku-20240307

# Groq configuration (fast inference - requires specific model names)
GROQ_API_KEY=your_groq_api_key
GROQ_MODEL=llama3-8b-8192  # Specific model for Groq
# Available models: llama3-8b-8192, llama3-70b-8192, mixtral-8x7b-32768, gemma-7b-it
# Check https://console.groq.com/docs/models for latest models

# OpenRouter configuration (multiple models - requires full model path)
OPENROUTER_API_KEY=your_openrouter_api_key
OPENROUTER_MODEL=anthropic/claude-3-sonnet-20240229  # Specific model for OpenRouter
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# Available models: anthropic/claude-3-sonnet-20240229, openai/gpt-4o-mini, meta-llama/llama-3.1-8b-instruct
# Check https://openrouter.ai/models for latest models and pricing

# Ollama configuration (local models - must be installed locally)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3:8b  # Specific model for Ollama
# Available models: llama3, llama3:8b, mistral, mixtral:8x7b, codellama, gemma:7b
# Install with: ollama pull llama3
# Check https://ollama.ai/library for available models

# Custom model configuration (enterprise/private APIs)
CUSTOM_MODEL_BASE_URL=https://your-custom-api.com/v1
CUSTOM_MODEL_NAME=your-model-name  # Specific model for custom provider
CUSTOM_MODEL_API_KEY=your_custom_api_key
CUSTOM_MODEL_HEADERS={"X-Custom-Header": "value"}

# =============================================================================
# BACKGROUND JOB PROCESSING
# =============================================================================
# Job queue provider: inprocess, celery, rq
JOB_QUEUE_PROVIDER=inprocess

# Celery configuration
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0
CELERY_TASK_SERIALIZER=json
CELERY_RESULT_SERIALIZER=json
CELERY_ACCEPT_CONTENT=["json"]

# Redis configuration (for Celery/RQ)
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# MEMORY LIFECYCLE CONFIGURATION
# =============================================================================
# Memory tier limits
VERY_NEW_LIMIT=20
MID_TERM_LIMIT=200
LONG_TERM_LIMIT=1000

# Memory tier timeouts (in days)
VERY_NEW_DAYS=10
MID_TERM_DAYS=365
LONG_TERM_DAYS=1095

# Compression settings
COMPRESSION_BATCH_SIZE=5
COMPRESSION_MAX_RETRIES=3
COMPRESSION_RETRY_DELAY=1.0

# =============================================================================
# AGENT-SPECIFIC CONFIGURATION
# =============================================================================
# Default agent configuration
DEFAULT_AGENT_TYPE=general
DEFAULT_AGENT_CONTEXT_WINDOW=5
DEFAULT_AGENT_MAX_TOKENS=4000

# Agent memory profiles
AGENT_MEMORY_PROFILES=general,conversational,analytical,creative

# =============================================================================
# RETRIEVAL CONFIGURATION
# =============================================================================
# Hybrid search settings
HYBRID_SEARCH_ENABLED=true
KEYWORD_SEARCH_WEIGHT=0.7
VECTOR_SEARCH_WEIGHT=0.3
MIN_RELEVANCE_SCORE=0.1
MAX_RETRIEVAL_ITEMS=10

# Search fallback settings
ENABLE_VECTOR_FALLBACK=true
FALLBACK_THRESHOLD=0.3
MIN_DB_RESULTS=2

# =============================================================================
# API CONFIGURATION
# =============================================================================
# API server settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_RELOAD=false

# CORS settings
CORS_ORIGINS=["http://localhost:3000", "http://localhost:8080"]
CORS_METHODS=["GET", "POST", "PUT", "DELETE", "OPTIONS"]
CORS_HEADERS=["*"]

# =============================================================================
# AUTHENTICATION & SECURITY
# =============================================================================
# JWT configuration
JWT_SECRET_KEY=your_super_secret_jwt_key_change_this_in_production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# API key configuration
API_KEY_HEADER=X-API-Key
API_KEY_LENGTH=32

# Security settings
ENABLE_RATE_LIMITING=true
RATE_LIMIT_REQUESTS_PER_MINUTE=100
ENABLE_AUDIT_LOGGING=true
AUDIT_LOG_FILE=logs/audit.log

# =============================================================================
# MONITORING & OBSERVABILITY
# =============================================================================
# Logging configuration
LOG_LEVEL=INFO
LOG_FORMAT=json
LOG_FILE=logs/memorizer.log
LOG_MAX_SIZE=100MB
LOG_BACKUP_COUNT=5

# Monitoring providers
MONITORING_PROVIDER=console

# Sentry configuration
SENTRY_DSN=your_sentry_dsn
SENTRY_ENVIRONMENT=development
SENTRY_TRACES_SAMPLE_RATE=0.1

# Prometheus configuration
PROMETHEUS_ENABLED=true
PROMETHEUS_PORT=9090
PROMETHEUS_PATH=/metrics

# Health check settings
HEALTH_CHECK_INTERVAL=30
HEALTH_CHECK_TIMEOUT=5

# =============================================================================
# PERFORMANCE & OPTIMIZATION
# =============================================================================
# Caching configuration
CACHE_PROVIDER=memory
CACHE_TTL_SECONDS=300
CACHE_MAX_SIZE=1000

# Redis cache configuration
REDIS_CACHE_URL=redis://localhost:6379/1
REDIS_CACHE_TTL=300

# Memory optimization
ENABLE_MEMORY_COMPRESSION=true
MEMORY_COMPRESSION_THRESHOLD=0.8
GARBAGE_COLLECTION_INTERVAL=3600

# =============================================================================
# DEVELOPMENT & TESTING
# =============================================================================
# Development settings
DEBUG=false
TESTING=false
MOCK_EXTERNAL_APIS=false

# Test database
TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5432/memorizer_test

# Test vector database
TEST_VECTOR_DB_PROVIDER=mock

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================
# Environment
ENVIRONMENT=development

# Docker settings
DOCKER_IMAGE_TAG=latest
DOCKER_REGISTRY=your_registry.com

# Kubernetes settings
K8S_NAMESPACE=memorizer
K8S_REPLICAS=3
K8S_RESOURCE_LIMITS_CPU=1000m
K8S_RESOURCE_LIMITS_MEMORY=2Gi

# =============================================================================
# INTEGRATION CONFIGURATION
# =============================================================================
# LangChain integration
LANGCHAIN_ENABLED=false
LANGCHAIN_API_KEY=your_langchain_api_key

# LlamaIndex integration
LLAMAINDEX_ENABLED=false
LLAMAINDEX_API_KEY=your_llamaindex_api_key

# AutoGPT integration
AUTOGPT_ENABLED=false
AUTOGPT_API_KEY=your_autogpt_api_key

# CrewAI integration
CREWAI_ENABLED=false
CREWAI_API_KEY=your_crewai_api_key

# =============================================================================
# CUSTOM AGENT CONFIGURATIONS
# =============================================================================
# E-commerce agent
ECOMMERCE_AGENT_ENABLED=false
ECOMMERCE_AGENT_CONTEXT_WINDOW=10
ECOMMERCE_AGENT_MEMORY_TTL=30

# Customer service agent
CUSTOMER_SERVICE_AGENT_ENABLED=false
CUSTOMER_SERVICE_AGENT_CONTEXT_WINDOW=15
CUSTOMER_SERVICE_AGENT_MEMORY_TTL=90

# Research agent
RESEARCH_AGENT_ENABLED=false
RESEARCH_AGENT_CONTEXT_WINDOW=20
RESEARCH_AGENT_MEMORY_TTL=365

# Creative agent
CREATIVE_AGENT_ENABLED=false
CREATIVE_AGENT_CONTEXT_WINDOW=8
CREATIVE_AGENT_MEMORY_TTL=180
